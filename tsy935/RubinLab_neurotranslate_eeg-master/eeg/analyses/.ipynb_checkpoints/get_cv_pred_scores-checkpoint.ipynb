{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import inspect\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '/share/pi/rubin/siyitang/eeg/output/SeizureNet/train/train-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0...\n",
      "F1: 0.6632688447835299\n",
      "Precision: 0.6666492519993309\n",
      "Recall: 0.6979257772060181\n",
      "Acc: 0.6732673267326733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.83      0.78       199\n",
      "           1       0.68      0.46      0.55        83\n",
      "           2       0.75      0.67      0.71         9\n",
      "           3       0.46      0.38      0.42        69\n",
      "           4       0.76      0.65      0.70        20\n",
      "           5       0.88      1.00      0.93        14\n",
      "           6       0.41      0.90      0.56        10\n",
      "\n",
      "    accuracy                           0.67       404\n",
      "   macro avg       0.67      0.70      0.66       404\n",
      "weighted avg       0.67      0.67      0.66       404\n",
      "\n",
      "Fold 1...\n",
      "F1: 0.7596842218239839\n",
      "Precision: 0.7444347853711156\n",
      "Recall: 0.7808891467382418\n",
      "Acc: 0.7326732673267327\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78       199\n",
      "           1       0.72      0.73      0.73        83\n",
      "           2       0.82      1.00      0.90         9\n",
      "           3       0.54      0.46      0.50        69\n",
      "           4       0.89      0.85      0.87        20\n",
      "           5       0.76      0.93      0.84        14\n",
      "           6       0.70      0.70      0.70        10\n",
      "\n",
      "    accuracy                           0.73       404\n",
      "   macro avg       0.74      0.78      0.76       404\n",
      "weighted avg       0.73      0.73      0.73       404\n",
      "\n",
      "Fold 2...\n",
      "F1: 0.7501204061708735\n",
      "Precision: 0.7489655151309286\n",
      "Recall: 0.7909355783204118\n",
      "Acc: 0.7331670822942643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80       198\n",
      "           1       0.62      0.78      0.69        83\n",
      "           2       0.90      1.00      0.95         9\n",
      "           3       0.62      0.47      0.53        68\n",
      "           4       0.83      0.50      0.62        20\n",
      "           5       0.93      1.00      0.96        13\n",
      "           6       0.53      1.00      0.69        10\n",
      "\n",
      "    accuracy                           0.73       401\n",
      "   macro avg       0.75      0.79      0.75       401\n",
      "weighted avg       0.74      0.73      0.73       401\n",
      "\n",
      "Fold 3...\n",
      "F1: 0.6466127864606366\n",
      "Precision: 0.632442988952452\n",
      "Recall: 0.6929205765492087\n",
      "Acc: 0.6458852867830424\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75       198\n",
      "           1       0.59      0.33      0.42        83\n",
      "           2       0.64      1.00      0.78         9\n",
      "           3       0.49      0.59      0.53        68\n",
      "           4       0.48      0.70      0.57        20\n",
      "           5       0.91      0.77      0.83        13\n",
      "           6       0.58      0.70      0.64        10\n",
      "\n",
      "    accuracy                           0.65       401\n",
      "   macro avg       0.63      0.69      0.65       401\n",
      "weighted avg       0.65      0.65      0.64       401\n",
      "\n",
      "Fold 4...\n",
      "F1: 0.6412131192659006\n",
      "Precision: 0.6366468318334381\n",
      "Recall: 0.6913434742279534\n",
      "Acc: 0.6591478696741855\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73       198\n",
      "           1       0.73      0.39      0.50        83\n",
      "           2       0.62      1.00      0.76         8\n",
      "           3       0.51      0.46      0.48        68\n",
      "           4       0.95      1.00      0.97        19\n",
      "           5       0.65      1.00      0.79        13\n",
      "           6       0.33      0.20      0.25        10\n",
      "\n",
      "    accuracy                           0.66       399\n",
      "   macro avg       0.64      0.69      0.64       399\n",
      "weighted avg       0.66      0.66      0.64       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_f1 = []\n",
    "all_precision = []\n",
    "all_recall = []\n",
    "all_acc = []\n",
    "\n",
    "for fold_idx in range(NUM_FOLDS):\n",
    "    print('Fold {}...'.format(fold_idx))\n",
    "    # Get true labels\n",
    "    curr_true_txt = '../data/fold' + str(fold_idx) + '_testSet_seizure_files.txt'   \n",
    "    with open(curr_true_txt, 'r') as f:\n",
    "        true_str = f.readlines()\n",
    "        \n",
    "    # Get predicted labels\n",
    "    preds_file = os.path.join(os.path.join(results_dir, 'fold_' + str(fold_idx)), 'test_prediction.csv')\n",
    "    df_preds = pd.read_csv(preds_file).dropna(how='all')\n",
    "    pred_labels = df_preds['seizure_class']\n",
    "    pred_files = df_preds['file']\n",
    "    \n",
    "    assert(len(true_str) == len(pred_files)) # sanity check they have the same length\n",
    "    \n",
    "    true_labels_list = []\n",
    "    pred_labels_list = []\n",
    "    for i in range(len(true_str)):\n",
    "        tup = true_str[i].strip(\"\\n\").split(\",\")\n",
    "        curr_true_file = tup[0] + '_' + tup[2]\n",
    "        \n",
    "        assert(curr_true_file == pred_files[i]) # double check the files are the same\n",
    "        \n",
    "        true_labels_list.append(int(tup[1]))\n",
    "        pred_labels_list.append(pred_labels[i])\n",
    "        \n",
    "    f1 = f1_score(y_true=true_labels_list, y_pred=pred_labels_list, average='macro')\n",
    "    all_f1.append(f1)\n",
    "    print('F1: {}'.format(f1))\n",
    "\n",
    "    precision = precision_score(y_true=true_labels_list, y_pred=pred_labels_list, average='macro')\n",
    "    all_precision.append(precision)\n",
    "    print('Precision: {}'.format(precision))\n",
    "    \n",
    "    recall = recall_score(y_true=true_labels_list, y_pred=pred_labels_list, average='macro')\n",
    "    all_recall.append(recall)\n",
    "    print('Recall: {}'.format(recall))\n",
    "\n",
    "    acc = accuracy_score(y_true=true_labels_list, y_pred=pred_labels_list)\n",
    "    all_acc.append(acc)\n",
    "    print('Acc: {}'.format(acc))\n",
    "\n",
    "    print(classification_report(y_true=true_labels_list, y_pred=pred_labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged F1: 0.6921798757009849\n",
      "Averaged precision: 0.6858278746574531\n",
      "Averaged recall: 0.7308029106083668\n",
      "Averaged acc: 0.6888281665621796\n"
     ]
    }
   ],
   "source": [
    "mean_f1 = np.mean(all_f1)\n",
    "print('Averaged F1: {}'.format(mean_f1))\n",
    "\n",
    "mean_precision = np.mean(all_precision)\n",
    "print('Averaged precision: {}'.format(mean_precision))\n",
    "\n",
    "mean_recall = np.mean(all_recall)\n",
    "print('Averaged recall: {}'.format(mean_recall))\n",
    "\n",
    "mean_acc = np.mean(all_acc)\n",
    "print('Averaged acc: {}'.format(mean_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (eeg)",
   "language": "python",
   "name": "eeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
