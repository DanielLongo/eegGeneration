{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jdunnmon/repos/metal')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting data location\n",
    "eeg_data_path = '/home/tsy935/Docs/RubinLab/Data/EEG/Reports'\n",
    "eeg_data_file = 'reports_unique_for_hl_mm.csv'\n",
    "data_path = os.path.join(eeg_data_path, eeg_data_file)\n",
    "\n",
    "# Loading data\n",
    "df_eeg = pd.read_csv(data_path, index_col=0).dropna(how='all')\n",
    "df_eeg = df_eeg.rename({'Note': 'note', 'Hand Label (1 for seizure, -1 for no seizure, 0 for unsure)': 'hand_label'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing a single EEGNote\n",
    "from eeg_utils import EEGNote\n",
    "\n",
    "noteObj = EEGNote(df_eeg['note_uuid'][100], df_eeg['note'][100])\n",
    "\n",
    "noteObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "from eeg_utils import parse_eeg_docs\n",
    "\n",
    "# Parsing documents -- note that 1 = abnormal, 2 = normal!\n",
    "\n",
    "eeg_note_dill = 'parsed_eeg_notes.dill'\n",
    "eeg_note_dill_path = os.path.join(eeg_data_path, eeg_note_dill)\n",
    "\n",
    "if os.path.exists(eeg_note_dill_path):\n",
    "    print('Loading pre-parsed EEG notes...')\n",
    "    with open(eeg_note_dill_path, 'rb') as af:\n",
    "        docs = dill.load(af)\n",
    "else:\n",
    "    print('Parsing EEG notes...')\n",
    "    docs = parse_eeg_docs(df_eeg, use_dask=False)\n",
    "    with open(eeg_note_dill_path,'wb') as af:\n",
    "        dill.dump(docs, af)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are docs with empty sections -- most look like they're not EEG reports!\n",
    "from eeg_utils import get_empty_docs\n",
    "empty_docs = get_empty_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing empty EEG docs\n",
    "eeg_docs = list(set(docs)-set(empty_docs))\n",
    "print(f'Number of EEG Reports with Sections: {len(eeg_docs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from eeg_utils import create_data_split\n",
    "\n",
    "# Shuffling and setting seed\n",
    "np.random.seed(1701)\n",
    "np.random.shuffle(eeg_docs)\n",
    "\n",
    "# Creating data split\n",
    "train_docs, dev_docs, test_docs = create_data_split(eeg_docs)\n",
    "docs_list = [train_docs, dev_docs, test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing dev/test label balance\n",
    "Y_dev = np.array([doc.gold_label for doc in dev_docs])\n",
    "Y_test = np.array([doc.gold_label for doc in test_docs])\n",
    "\n",
    "dev_balance= np.sum(Y_dev == 1)/len(Y_dev)\n",
    "test_balance = np.sum(Y_test == 1)/len(Y_test)\n",
    "\n",
    "print(f'Dev positive percentage: {dev_balance}')\n",
    "print(f'Test positive percentage: {test_balance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from eeg_lfs import *\n",
    "from eeg_utils import get_section_with_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.analysis import single_lf_summary, confusion_matrix\n",
    "\n",
    "# Testing single LF\n",
    "lf_test = lf_impression_section_positive\n",
    "\n",
    "# Computing labels\n",
    "Y_lf = np.array([lf_test(doc) for doc in dev_docs])\n",
    "single_lf_summary(Y_lf, Y=Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix\n",
    "conf = confusion_matrix(Y_dev, Y_lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [\n",
    "    lf_normal_interp_not_seizure,\n",
    "    lf_abnormal_interp_with_seizure,\n",
    "    lf_findall_interp_with_seizure,\n",
    "    lf_findall_abnl_interp_without_seizure,\n",
    "    lf_abnl_interp_negexsp_seizure,\n",
    "    lf_findall_interp_negex_seizure,\n",
    "    lf_seizure_section,\n",
    "    lf_impression_section_negative,\n",
    "    lf_impression_section_positive,\n",
    "    lf_spikes_in_impression,\n",
    "    lf_extreme_words_in_impression\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "from eeg_utils import evaluate_lf_on_docs, create_label_matrix\n",
    "import pickle\n",
    "\n",
    "# Resetting LFs\n",
    "clobber_lfs = True\n",
    "Ls_file = 'Ls_0p3.pkl'\n",
    "Ys_file = 'Ys_0p3.pkl'\n",
    "\n",
    "# Get lf names\n",
    "lf_names = [lf.__name__ for lf in lfs]\n",
    "\n",
    "# Loading Ls if they exist\n",
    "\n",
    "Ls = []\n",
    "Ys = []\n",
    "if clobber_lfs or (not os.path.exists(Ls_file)):\n",
    "    print('Computing label matrices...')\n",
    "    for i, docs in enumerate([train_docs, dev_docs, test_docs]):\n",
    "        Ls.append(create_label_matrix(lfs,docs))  \n",
    "    with open(Ls_file,'wb') as af:\n",
    "        pickle.dump(Ls, af)\n",
    "    \n",
    "    print('Creating label vectors...')\n",
    "    Ys = [[],Y_dev, Y_test]\n",
    "    with open(Ys_file,'wb') as af:\n",
    "        pickle.dump(Ls, af)\n",
    "else:\n",
    "    print('Loading pre-computed label matrices...')\n",
    "    with open(Ls_file,'rb') as af:\n",
    "        Ls=pickle.load(af) \n",
    "        \n",
    "\n",
    "# Create label matrices\n",
    "#Ls = []\n",
    "#for i, docs in enumerate([train_docs, dev_docs, test_docs]):\n",
    "#    Ls.append(create_label_matrix(lfs,docs)) \n",
    "    \n",
    "# Create Ys\n",
    "Ys = [[], Y_dev, Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.analysis import lf_summary\n",
    "\n",
    "# Analyzing LF stats\n",
    "df_lf = lf_summary(Ls[1], Y=Y_dev, lf_names=lf_names)\n",
    "df_lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.contrib.visualization.analysis import view_label_matrix, view_overlaps\n",
    "\n",
    "# Viewing label matrix\n",
    "view_label_matrix(Ls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  metal.contrib.visualization.analysis import view_conflicts\n",
    "\n",
    "# Viewing conflicts\n",
    "view_conflicts(Ls[1], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.label_model import LabelModel\n",
    "from metal.utils import LogWriter\n",
    "from metal.tuners import RandomSearchTuner\n",
    "\n",
    "# Creating metal label model\n",
    "#label_model = LabelModel(k=2, seed=123)\n",
    "\n",
    "# Creating search space\n",
    "search_space = {\n",
    "        'l2': {'range': [0.0001, 0.1], 'scale':'log'},           # linear range\n",
    "        'lr': {'range': [0.0001, 0.01], 'scale': 'log'},  # log range\n",
    "        }\n",
    "\n",
    "searcher = RandomSearchTuner(LabelModel, log_dir='./run_logs',\n",
    "               log_writer_class=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training label model\n",
    "label_model = searcher.search(search_space, (Ls[1],Ys[1]), \\\n",
    "        train_args=[Ls[0]], init_args=[],\n",
    "        init_kwargs={'k':2, 'seed':123}, train_kwargs={'n_epochs':100},\n",
    "        max_search=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best model\n",
    "searcher._save_best_model(label_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting scores\n",
    "scores = label_model.score((Ls[1], Ys[1]), metric=['accuracy','precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.label_model.baselines import MajorityLabelVoter\n",
    "\n",
    "# Checking if we beat majority vote\n",
    "mv = MajorityLabelVoter(seed=123)\n",
    "scores = mv.score((Ls[1], Ys[1]), metric=['accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting probabilistic training labels\n",
    "# Y_train_ps stands for \"Y[labels]_train[split]_p[redicted]s[oft]\"\n",
    "Y_train_ps = label_model.predict_proba(Ls[0])\n",
    "Y_dev_ps = label_model.predict_proba(Ls[1])\n",
    "Y_test_ps = label_model.predict_proba(Ls[2])\n",
    "Y_ps = [Y_train_ps, Y_dev_ps, Y_test_ps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running some analysis \n",
    "from metal.contrib.visualization.analysis import plot_predictions_histogram\n",
    "Y_dev_p = label_model.predict(Ls[1])\n",
    "plot_predictions_histogram(Y_dev_p, Ys[1], title=\"Label Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  metal.contrib.visualization.analysis  import plot_probabilities_histogram\n",
    "\n",
    "# Looking at probability histogram for training labels\n",
    "plot_probabilities_histogram(Y_dev_ps[:,0], title=\"Probablistic Label Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  metal.analysis import confusion_matrix\n",
    "\n",
    "# Printing confusion matrix\n",
    "cm = confusion_matrix(Ys[1], Y_dev_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.contrib.featurizers.embedding_featurizer import TrainableEmbeddingFeaturizer\n",
    "\n",
    "# Defining featurizer\n",
    "# TODO: use a different one for IdentityModule!\n",
    "featurizer = TrainableEmbeddingFeaturizer()\n",
    "\n",
    "# Getting raw input data\n",
    "Xs = [[doc.tokens for doc in doc_split] for doc_split in [train_docs, dev_docs, test_docs]]\n",
    "\n",
    "# Flattening input data and getting lengths for unflattening\n",
    "X_flat = Xs[0]+Xs[1]+Xs[2]\n",
    "lens = [len(X) for X in Xs]\n",
    "lens = np.cumsum(lens)\n",
    "\n",
    "# Fitting featurizer\n",
    "featurizer.fit(X_flat, min_freq=100)\n",
    "\n",
    "# Creating transformed data\n",
    "X_trans = featurizer.transform(X_flat).float()\n",
    "\n",
    "# Unflattening data\n",
    "X_trans = [X_trans[:lens[0]], X_trans[lens[0]:lens[1]], X_trans[lens[1]:lens[2]]]\n",
    "\n",
    "# Print embedding size\n",
    "print(f'Embedding size: {len(X_trans[0][0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from metal.end_model import EndModel\n",
    "from metal.modules import LSTMModule, IdentityModule\n",
    "\n",
    "# LSTM parameters\n",
    "hidden_size = 50\n",
    "embed_size = 100\n",
    "vocab_size = len(X_trans[0][0]) # Update Metal to handle this more gracefully!\n",
    "input_module = LSTMModule(embed_size, hidden_size, vocab_size = vocab_size)\n",
    "\n",
    "# Identity parameters\n",
    "#feature_size = len(X_trans[0][0])\n",
    "#hidden_size = 1000\n",
    "#input_module = IdentityModule()\n",
    "\n",
    "# Defining end model\n",
    "end_model = EndModel([embed_size,100,2], input_module=input_module, seed=123, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from metal.utils import MetalDataset\n",
    "\n",
    "# Training end model\n",
    "train_data = (X_trans[0].long(), torch.Tensor(Y_train_ps))\n",
    "dev_data = (X_trans[1].long(), torch.Tensor(Ys[1]))\n",
    "batch_size = 256\n",
    "\n",
    "train_data = DataLoader(MetalDataset(*train_data), shuffle=True, batch_size=batch_size)\n",
    "dev_data = DataLoader(MetalDataset(*dev_data), shuffle=True, batch_size = batch_size)\n",
    "\n",
    "end_model.train_model(train_data, dev_data=dev_data, l2=0.00001, lr=0.001, batch_size=256, \n",
    "                num_workers=8, n_epochs=10, print_every=1, validation_metric='accuracy')\n",
    "\n",
    "# Emptying cuda cache (add this to metal?)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluating performance\n",
    "print(\"Label Model:\")\n",
    "score = label_model.score((Ls[2], Ys[2]), metric=['accuracy','precision', 'recall', 'f1'])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"End Model:\")\n",
    "score = end_model.score((X_trans[2].long(), Ys[2]), metric=['accuracy', 'precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dest = os.path.dirname(searcher.save_path)\n",
    "splits = ['train','dev','test']\n",
    "save_csvs = True\n",
    "results_df = {}\n",
    "for ind, split in enumerate(splits):\n",
    "    # Evaluating scores and writing to file\n",
    "    doc_ids = [doc.doc_id for doc in docs_list[ind]]\n",
    "    gm_marginals = [y for y in Y_ps[ind]]\n",
    "\n",
    "    # Creating dataframe\n",
    "    df_dict = {'id': doc_ids, 'gm_marginals':gm_marginals}\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    results_df[split] = df\n",
    "    \n",
    "    # Writing dataframe\n",
    "    if save_csvs:\n",
    "        results_df[split].to_csv(os.path.join(save_dest, f'metal_results_{split}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SANDBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (eeg)",
   "language": "python",
   "name": "eeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
